import re
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import linregress, ttest_rel
import pandas as pd
import matplotlib as mpl

# ── 1)  paste your raw console output below ────────────────────────────────
# raw_bethe = """
# 1 0.01506428972203609 0.3743539756775976
# 2 0.014999974108994374 0.3738881560931656
# 3 0.014953344032220008 0.3742788316733964
# 4 0.01501708073393789 0.3743567168143094
# 5 0.01502704931985087 0.37435894222419547
# 6 0.015023847782817984 0.3738036941731841
# 7 0.014969262188608258 0.3735409116409571
# 8 0.014776875515098478 0.37217790402747514
# 9 0.014636855061610799 0.3716828872309318
# 10 0.014405242779420088 0.3699521179780447
# 11 0.014203882279367317 0.36606959676775325
# 12 0.01406746155465988 0.3641746608894518
# 13 0.014012287606312084 0.3616976632031376
# 14 0.013991836239614884 0.3601705373066741
# 15 0.01393048543937955 0.3568494509385257
# 16 0.013827276870763466 0.35448386659474634
# 17 0.013867810774660351 0.35312844071861954
# 18 0.013741697899339444 0.35066758933838554
# 19 0.01366718897802596 0.34808514536730073
# 20 0.013794662709761804 0.34749577044218716
# 21 0.013720835667149123 0.3432396349128155
# 22 0.013740639777948222 0.3426712579220301
# 23 0.013701851996202514 0.3412749209341404
# 24 0.013469805776873825 0.3377970715685974
# 25 0.013444543567334707 0.3354460609984474
# 26 0.013063004201678552 0.3293740931331492
# 27 0.012748256330576064 0.3265691117317184
# 28 0.0126873954315521 0.32288931019410605
# 29 0.012830114702347373 0.32321457664450914
# 30 0.012867353437306557 0.32320796711130173
# 31 0.012957307217440464 0.32244496827241226
# 32 0.012775445246313393 0.32109737846257147
# 33 0.012860504029497186 0.32100077687727985
# 34 0.012826496858930517 0.3214217439618472
# 35 0.012896924559841169 0.3205308575040298
# 36 0.0128529591929533 0.3203673956038887
# 37 0.012833855492946748 0.31969936091019785
# 38 0.012860547692733386 0.3193176093359397
# 39 0.012827782592654784 0.3195691991394682
# 40 0.012930659243105967 0.3193756551567811
# 41 0.012890466790654472 0.3195311204386355
# 42 0.012922301973697902 0.31900735009262426
# 43 0.012878203955201206 0.3188642689502925
# 44 0.012910950628397205 0.3194666697721191
# 45 0.01287841602080263 0.3180712155134087
# 46 0.01291507420216621 0.31757476264726436
# 47 0.012850940228192975 0.3176275626802818
# 48 0.01279527399492337 0.3167634192306856
# 49 0.012931121718064476 0.3188837047680463
# 50 0.012891697663946051 0.31758550121693513
# 51 0.012872972495886975 0.31695502376509455
# 52 0.012958782234665487 0.3179413430884242
# 53 0.01285901794258008 0.31698952526965696
# 54 0.012820504736366416 0.3163852844462687
# 55 0.012825474219581417 0.317043350584035
# 56 0.012868569923535094 0.3170456264480727
# 57 0.012926767075629206 0.3177824984346709
# 58 0.012850518253062385 0.3162869771570921
# 59 0.012899117754353285 0.3172580034934891
# 60 0.012940519386527068 0.31758888821562875
# 61 0.012877741180429404 0.3164808618847472
# 62 0.012871979509019705 0.3172560779187417
# 63 0.012906894138315616 0.31738346328740724
# 64 0.012796476498588867 0.31616103010292324
# 65 0.012921614316658458 0.31650826200019466
# 66 0.012930065409488338 0.3183253109774005
# 67 0.012800462779038789 0.3161502030143584
# 68 0.012882337603443414 0.3170586632922752
# 69 0.012917862833335125 0.31734047826853307
# 70 0.012906315147686101 0.31647766754541695
# 71 0.012910940632413673 0.31747974711350424
# 72 0.01284996331584025 0.3159531092069274
# 73 0.012819081256866764 0.31547232305083084
# 74 0.012926824444017791 0.3173410178458296
# 75 0.012889390635203331 0.3169859704114292
# 76 0.01289598696485326 0.31686177937858867
# 77 0.01290439444915217 0.31642711846468075
# 78 0.012853141687572602 0.31482654559516066
# 79 0.012883617186836227 0.3162522522160878
# 80 0.012861202515972218 0.31599307332392135
# 81 0.01288099823144894 0.31675224863610224
# 82 0.01289257526950309 0.3171466006010351
# 83 0.012867426462525335 0.31653903654536475
# 84 0.012888426922089604 0.31646199962175336
# 85 0.012965349721700986 0.3172170847068188
# 86 0.012926740394726336 0.3152572571873258
# 87 0.012897066037435087 0.3159188350896555
# 88 0.01283844944223699 0.31502641222669436
# 89 0.012868530111552076 0.315437407398104
# 90 0.012928808873185583 0.3164728177703408
# 91 0.012873186096534752 0.3150392393213141
# 92 0.012618777936728406 0.3112336699779287
# 93 0.01263706641448704 0.31154291794172895
# 94 0.012702636625983142 0.31260998408690593
# 95 0.01256007610965886 0.30970180697805677
# 96 0.01265676983438912 0.31105477469425896
# 97 0.012597635270475967 0.3103140132419859
# 98 0.012607548129271175 0.3111660328607642
# 99 0.01260295434403472 0.310468202625026
# 100 0.012680169020471161 0.31144499529131914
# 101 0.012654373659804896 0.31114750634349897
# 102 0.012667136491952337 0.31183240617969143
# 103 0.012619916136940124 0.31125814789613426
# 104 0.012613078866184372 0.31017036424157524
# 105 0.012634773765875617 0.3110429867062633
# 106 0.012649011782034134 0.3110150619709696
# 107 0.012601766837448954 0.3105736365322007
# 108 0.01261881473274728 0.3110141651280935
# 109 0.012679127870366902 0.3113739877217294
# 110 0.012661531474937034 0.311648690244263
# 111 0.012628861465065179 0.31056946601595603
# 112 0.012737036284887409 0.31187640442701337
# 113 0.012595593497618202 0.30999535436011655
# 114 0.012670696571982993 0.31132910423788673
# 115 0.012658983690772604 0.3111698018086752
# 116 0.012704664694470033 0.3117224648005231
# 117 0.0126060667606475 0.3107710657717691
# 118 0.012668252886169209 0.3116132267017371
# 119 0.012607037252630899 0.3103158729788763
# 120 0.01260518542357839 0.3108746638499712
# 121 0.012671069454124044 0.3117830330900006
# 122 0.012632548750636756 0.3105179180095743
# 123 0.012467004251842736 0.3075951705267866
# 124 0.012410507660959485 0.3070104397591997
# 125 0.012498944410826694 0.3074704520788132
# 126 0.012506137460571706 0.307761591793197
# 127 0.012508332791819654 0.3080819860720863
# 128 0.012309319932217278 0.3061160198777052
# 129 0.01248098455830213 0.30812101942789116
# 130 0.01247314816855206 0.30706294589210165
# 131 0.012469215940874532 0.3078616749140968
# 132 0.012406505699351238 0.3063135355998985
# 133 0.012482220566194956 0.30768996995292786
# 134 0.012326467067210068 0.3048263325963958
# 135 0.012210083753621962 0.3031059087880423
# 136 0.012278545719296825 0.3044055105639182
# 137 0.01227469153560189 0.303824376155012
# 138 0.012289930078884384 0.304501664346312
# 139 0.012302154719349749 0.30413341614049577
# 140 0.01232327180779857 0.3048747218391903
# 141 0.012243870361418519 0.3034118488158845
# 142 0.012132806765954896 0.30076624362052184
# 143 0.012139454884468605 0.30102019387336226
# 144 0.01215445596631768 0.30055580187045355
# 145 0.012178533441505707 0.301568447766326
# 146 0.012172432575704013 0.3012054979034328
# 147 0.012181675148820934 0.3018852549889828
# 148 0.012121230608741972 0.30086861276200577
# 149 0.012201790500026883 0.30177736738492716
# 150 0.012245266108459817 0.30223047907022405
# slope = -1.2214540845872535e-05
# """

raw_motif = """
1 0.007146461568448717 0.14479590917783036
2 0.007162753910718703 0.14505305252070108
3 0.007144003702145812 0.1447616781208346
4 0.007162181177458381 0.1450430686639017
5 0.007145254086095728 0.14476838762549213
6 0.00715020745386421 0.14486406380256486
7 0.007132764835450103 0.14472189534151983
8 0.007095943870388948 0.1444039065721407
9 0.007042843266559139 0.14395680883785208
10 0.006979228615201026 0.14343486225865823
11 0.006907129017256494 0.14285680516347574
12 0.006820019261136717 0.1420360033579835
13 0.0067286703522809336 0.14112735372813892
14 0.0066089108461655725 0.13974560316903695
15 0.006540787116416275 0.1391414015574602
16 0.00643492894475366 0.13797738016296293
17 0.006287428506590193 0.13631976026769482
18 0.0061579450009418306 0.13475941506487607
19 0.0060918200623376354 0.13388049506435648
20 0.005942049325997753 0.1319598155857179
21 0.005846680579560584 0.13071050546033583
22 0.00578872624863075 0.1297029156742987
23 0.005646113043732035 0.1278919146179628
24 0.0055532861655879515 0.1265946827587434
25 0.005472988811314437 0.12526727602016238
26 0.005342061504556747 0.12317163698021628
27 0.005255930408822819 0.12213849524590559
28 0.005136477829346372 0.12041109507105777
29 0.00503094602777354 0.11872175989226381
30 0.004967022802614636 0.11764491945244694
31 0.004920123218848625 0.11669054889195456
32 0.0048572772594062 0.11542348118714908
33 0.004832840296359446 0.11487361143160196
34 0.004763749521886369 0.11358929367169149
35 0.004757473344449153 0.11309706388784731
36 0.004675432370753529 0.11224228727212364
37 0.0046105143095294294 0.11118899728688264
38 0.004539804578586579 0.10987519405786532
39 0.004416694775288437 0.10794542768389456
40 0.004356875659917639 0.10678552075422869
41 0.004347614541674476 0.10645753655597576
42 0.004290459338771846 0.10529695642505238
43 0.004234179377640646 0.10409803686037213
44 0.004193439424268659 0.1033839806858859
45 0.004211700512636382 0.10334347170544836
46 0.0041685461810705 0.10264055934629661
47 0.004126069782113438 0.10172312865852291
48 0.004104596888410617 0.10119108904198966
49 0.004106864565017787 0.10099132701469546
50 0.004134036013180096 0.10138180258150988
51 0.004133784844142974 0.10123522776501834
52 0.004091775423123704 0.10027060341143677
53 0.004086591887133646 0.10020230401479802
54 0.0040275337794699806 0.09933691030785269
55 0.0040081897526379655 0.09889343031705616
56 0.003982023968698692 0.09832843635682413
57 0.003948144850573373 0.09767088103310485
58 0.003932125144603946 0.0973431105895203
59 0.003934758862626224 0.0971824933982582
60 0.003945412476271523 0.09726093870208342
61 0.003957103801194526 0.09743679759493619
62 0.003897477904107754 0.09611255791700113
63 0.0039225718900048985 0.09654386214673293
64 0.003903377250859915 0.09610891237089514
65 0.0039050456048345884 0.09618048712827894
66 0.0038493750097352156 0.09505889676638431
67 0.003810195127291746 0.09434819288800302
68 0.00376016955434185 0.0942702059017064
69 0.0036883713895002174 0.09273327301677028
70 0.0036623399611349374 0.09219479655116977
71 0.0036854505645073183 0.0926138115019849
72 0.0036351142640113432 0.09168372532214761
73 0.003661452438642575 0.09232887032410192
74 0.0036523445003606328 0.09210555856380521
75 0.0036162719666713093 0.09123740421100814
76 0.0035929169066081204 0.09078511446381853
77 0.0035958037365818618 0.09083877819495685
78 0.0035562119888173727 0.08983219929155377
79 0.0035423438036378476 0.08957090767664158
80 0.0035218671094754926 0.0891158503539564
81 0.003561944693100219 0.08986465551875299
82 0.003529631685222816 0.08905479133658742
83 0.003523203790500417 0.08898186241913605
84 0.003518669076283135 0.08878976512856317
85 0.003516763546745755 0.08874456057987336
86 0.0035158055192203567 0.08872808185103023
87 0.0034820753607134915 0.08804842872118854
88 0.0034983567478867918 0.08827315558829252
89 0.0035115855831284824 0.08876089225781525
90 0.003412870574063574 0.08670159725964485
91 0.003440571750932415 0.08703259038898535
92 0.0034821861533180978 0.08818446447226143
93 0.0034112065088726874 0.08671925765100111
94 0.003468273966018156 0.08770447248865693
95 0.0034178358761365315 0.08658297476276641
96 0.0034022520806583135 0.08631615836944895
97 0.0033294047505782885 0.08539280072315168
98 0.0033216442680014794 0.0852044492703981
99 0.0033369826757612647 0.08561869584788576
100 0.0033339050968030776 0.08547841329913866
101 0.0032477124402497384 0.08353533650036787
102 0.003249093556865727 0.08352826921823343
103 0.0032005484650463726 0.0827591237278735
104 0.0032350981441183787 0.08349313650806084
105 0.0032614798826308884 0.08412176072097131
106 0.003245500044394051 0.08360465738177128
107 0.003186964853178859 0.08231418841958996
108 0.0031872662174028 0.08226588570961643
109 0.0031854027239075337 0.08230392039525634
110 0.0031410519176985254 0.08126531801070162
111 0.003173248410284797 0.08195717242302866
112 0.003067766962340462 0.08034732636003498
113 0.003070885938407618 0.08067350396071658
114 0.0030290640019251646 0.07949917611158468
115 0.002994509314242137 0.0787170734429636
116 0.0030307844468471274 0.07949316907358431
117 0.003033746460680091 0.0794957741091565
118 0.002992913415487995 0.07851896427244762
119 0.0030417887023626274 0.07970549043233453
120 0.0029868161190790563 0.07865488979397779
121 0.0030075864475076442 0.0788279867233786
122 0.0029894313480753405 0.07846600347151118
123 0.003002847191402092 0.07860380367956368
124 0.003004397842701511 0.07864712015753696
125 0.002974379748603027 0.07807686926979861
126 0.0029589164105070704 0.0777114568540464
127 0.0029792584768431043 0.07799977872735614
128 0.002973242291260061 0.0780085537747851
129 0.0029971007957253462 0.0785050650074822
130 0.0029608493883570997 0.07768756092449804
131 0.0029363777289969613 0.07715702083555451
132 0.0028916856416666623 0.07647098791429185
133 0.002926079195096681 0.07727436741694228
134 0.0029036619472401193 0.07660353252159606
135 0.002905132613169304 0.07679919689612293
136 0.002891933564842796 0.0764927623719439
137 0.002835318274263951 0.07560577164091781
138 0.002777574792464337 0.07451801158697441
139 0.0027824380480777184 0.07475585346439034
140 0.0027479719265833466 0.07394359637470764
141 0.002760512378614364 0.07447335525999667
142 0.002753000709224137 0.07426318860343514
143 0.002756805136821508 0.07430508674710294
144 0.002772366736964683 0.07469398873125038
145 0.002793329399420316 0.07538335661893587
146 0.00277450573601838 0.0747796864350016
147 0.0027051043762813445 0.07317046564090565
148 0.00275548616844229 0.07430429450297099
149 0.002745190145969692 0.07388369088859525
150 0.002748918781457948 0.07410304996191909
slope = -2.6867686171784186e-05
"""
# ───────────────────────────────────────────────────────────────────────────

# 2) pull out iter#, noise (2nd number)
iters, noise = [], []
for m in re.finditer(r'^\s*(\d+)\s+([\d.]+)', raw_motif, re.M):
    iters.append(int(m.group(1)))
    noise.append(float(m.group(2)))

iters = np.asarray(iters)
noise = np.asarray(noise)

# ---------------------------------------------------- rolling μ ± σ (window w)
w = 5                              # centred window length (odd preferred)
s  = pd.Series(noise)
mu  = s.rolling(w, center=True, min_periods=1).mean().to_numpy()
std = s.rolling(w, center=True, min_periods=1).std(ddof=0).fillna(0).to_numpy()

fig, ax = plt.subplots(figsize=(20,20))
ax.grid(True, color='lightgrey', linestyle='-', linewidth=0.5, alpha=0.5)
ax.plot(iters, noise, marker='o', ms=3, label='Noise', color='red', linewidth=4)
ax.fill_between(iters, mu-std, mu+std,
                color='tab:grey', alpha=0.25, label='±1 σ')

# Increase tick label sizes
ax.tick_params(axis='both', which='major', labelsize=40)

# Add more tick markers
ax.xaxis.set_major_locator(plt.MaxNLocator(10))
ax.yaxis.set_major_locator(plt.MaxNLocator(10))

ax.set_xlabel('k', fontsize=60)
ax.set_ylabel(r'Noise metric $\mathcal{N}^k$', fontsize=60)
ax.set_title(r'Noise metric during GeoDe-MASO Run', fontsize=60, pad=12)
ax.legend(fontsize=60)
plt.tight_layout()
plt.savefig('experiments/convergence/noise_metric.png')
plt.close()

def noise_significance(iters, noise, *, burn_in=0.1, tail_fraction=0.1):
    """
    Parameters
    ----------
    iters  : 1-D array-like of ints      (iteration numbers)
    noise  : 1-D array-like of floats    (GN-MSE or similar)
    burn_in        : first fraction of points to ignore for the paired-t test
    tail_fraction  : last fraction of points to compare against the start
    
    Returns
    -------
    results : dict
        slope         – OLS slope per iteration
        p_slope       – two-sided p-value that slope ≠ 0
        mean_start    – mean noise in first window
        mean_end      – mean noise in last  window
        t_stat, p_t   – paired t-test of start vs end
    """
    iters = np.asarray(iters)
    noise = np.asarray(noise)
    
    # — 1) linear regression over the whole series ---------------------
    slope, *_ , p_slope, stderr = linregress(iters, noise)
    
    # — 2) compare start vs end windows --------------------------------
    n = len(noise)
    k  = int(max(3, round(tail_fraction * n)))     # at least 3 points
    j  = int(round(burn_in * n))
    
    start  = noise[j : j + k]
    end    = noise[-k:]
    
    # paired t-test (H0: mean(start) = mean(end))
    t_stat, p_t = ttest_rel(start, end)
    
    return dict(
        slope      = slope,
        p_slope    = p_slope,
        mean_start = float(start.mean()),
        mean_end   = float(end.mean()),
        t_stat     = float(t_stat),
        p_t        = p_t,
    )

res = noise_significance(iters, noise)
print(res)